{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrics Based Song Recommendation\n",
    "\n",
    "**The idea** is to match a song to some textual context based on valency, category and text similarity using song lyrics<br />\n",
    "**Valency:** pos, neg, neu<br />\n",
    "**Categories:** adventure, hobbies, humor, mystery, romance<br />\n",
    "**Similarity metrics:** cosine, WordNet word similarity or others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>seq</th>\n",
       "      <th>song</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>No, no\\r\\nI ain't ever trapped out the bando\\r...</td>\n",
       "      <td>Everyday</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>The drinks go down and smoke goes up, I feel m...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>She don't live on planet Earth no more\\r\\nShe ...</td>\n",
       "      <td>The Otherside</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>Trippin' off that Grigio, mobbin', lights low\\...</td>\n",
       "      <td>Pinot</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I see a midnight panther, so gallant and so br...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        artist  \\\n",
       "0           0  Elijah Blake   \n",
       "1           1  Elijah Blake   \n",
       "2           2  Elijah Blake   \n",
       "3           3  Elijah Blake   \n",
       "4           4  Elijah Blake   \n",
       "\n",
       "                                                 seq                song  \\\n",
       "0  No, no\\r\\nI ain't ever trapped out the bando\\r...            Everyday   \n",
       "1  The drinks go down and smoke goes up, I feel m...    Live Till We Die   \n",
       "2  She don't live on planet Earth no more\\r\\nShe ...       The Otherside   \n",
       "3  Trippin' off that Grigio, mobbin', lights low\\...               Pinot   \n",
       "4  I see a midnight panther, so gallant and so br...  Shadows & Diamonds   \n",
       "\n",
       "   label  \n",
       "0   0.63  \n",
       "1   0.63  \n",
       "2   0.24  \n",
       "3   0.54  \n",
       "4   0.37  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"D:\\ML\\Datasets\\labeled_lyrics_cleaned.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>song</th>\n",
       "      <th>valency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54911</td>\n",
       "      <td>Simon &amp;  Milo</td>\n",
       "      <td>Hello, this is Stacy, the computer\\nGood morni...</td>\n",
       "      <td>www.nevergetoveryou</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82479</td>\n",
       "      <td>Hippo Campus</td>\n",
       "      <td>See how the western kids\\r\\nHave silicon insid...</td>\n",
       "      <td>western kids</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82478</td>\n",
       "      <td>Hippo Campus</td>\n",
       "      <td>Wisconsin pines, collaborating with the day gl...</td>\n",
       "      <td>way it goes</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82477</td>\n",
       "      <td>Hippo Campus</td>\n",
       "      <td>I see meaning where you don't, where you don't...</td>\n",
       "      <td>vines</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82476</td>\n",
       "      <td>Hippo Campus</td>\n",
       "      <td>My thoughts are a battlefield of sub-surreal a...</td>\n",
       "      <td>vacation</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         artist                                             lyrics  \\\n",
       "0  54911  Simon &  Milo  Hello, this is Stacy, the computer\\nGood morni...   \n",
       "1  82479   Hippo Campus  See how the western kids\\r\\nHave silicon insid...   \n",
       "2  82478   Hippo Campus  Wisconsin pines, collaborating with the day gl...   \n",
       "3  82477   Hippo Campus  I see meaning where you don't, where you don't...   \n",
       "4  82476   Hippo Campus  My thoughts are a battlefield of sub-surreal a...   \n",
       "\n",
       "                  song  valency  \n",
       "0  www.nevergetoveryou     0.68  \n",
       "1         western kids     0.52  \n",
       "2          way it goes     0.52  \n",
       "3                vines     0.66  \n",
       "4             vacation     0.55  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop, rename columns, remove duplicates and reset index\n",
    "\n",
    "data.drop(labels=\"Unnamed: 0\", axis=1, inplace=True)\n",
    "data.rename(columns={\"seq\": \"lyrics\", \"label\": \"valency\"}, inplace=True)\n",
    "\n",
    "data.sort_values(by=['song', 'valency'], ascending=False, inplace=True) # to keep highest value valency \n",
    "data = data.drop_duplicates(subset='lyrics')\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x) # round everything to 2 decimal places\n",
    "data = data.reset_index() \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence segmentation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you Come close And hold me tight You feel The heart that beats For you  And if You dear Could read my mind Oh you Would know my love Is true   Words can`t say how much I love you Words can`t say how much I care I need you `n I need your love Like I need to breathe the air  Faith And trust Give both a try So you Will see that is The key  I swear True love Will never die So please Believe in you And me '"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string # removes punctuation\n",
    "\n",
    "pattern = r'\\[.*?\\]' # remove square brackets and contents\n",
    "\n",
    "song = data['lyrics'][3334].replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"\\t\", \"\").replace(\"  \", \" \").strip()\n",
    "#song = song.translate(str.maketrans('', '', string.punctuation)) # removes punctuation\n",
    "song = re.sub(pattern, '', song)\n",
    "song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsplit import NNSplit\n",
    "\n",
    "splitter = NNSplit.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and if you dear could read my mind oh you would know my love is true',\n",
       " 'faith and trust give both a try so you will see that is the key',\n",
       " 'i need to breathe the air',\n",
       " 'i need you `n',\n",
       " 'i need your love like',\n",
       " 'i swear true love will never die so please believe in you and me',\n",
       " 'if you come close and hold me tight you feel the heart that beats for you',\n",
       " 'words can`t say how much i love you words can`t say how much i care']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = splitter.split([song.lower()])[0]\n",
    "song_sents = [str(x).strip() for x in splits]\n",
    "song_sents_sorted = sorted(set(song_sents))\n",
    "\n",
    "song_sents_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Part-of-speech tagging (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('if', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('come', 'VBP'),\n",
       " ('close', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('hold', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('tight', 'JJ'),\n",
       " ('you', 'PRP'),\n",
       " ('feel', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('heart', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('beats', 'VBZ'),\n",
       " ('for', 'IN'),\n",
       " ('you', 'PRP')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "pos_tag(word_tokenize(song_sents[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Attempt sentiment classification using Vader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse valency and extract compound score:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def valency(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(text)\n",
    "    #ss.pop('compound')\n",
    "    compound_score = ss.get('compound')\n",
    "    if compound_score > 0.3 and compound_score <= 1:\n",
    "        valency = 'positive'\n",
    "    elif compound_score >= -1 and compound_score < -0.3:\n",
    "        valency = 'negative'\n",
    "    else:\n",
    "        valency = 'neutral'\n",
    "    return valency, compound_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load song lyrics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every time I turn my back I get the feeling that\r\n",
      "I'm 'bout to take a shot to the skully with a bat\r\n"
     ]
    }
   ],
   "source": [
    "lyrics = data.at[201, 'lyrics'].strip()\n",
    "#text = tokenize.sent_tokenize(lyrics)[0]\n",
    "print(lyrics[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', -0.8814)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valency(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User situtation test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('positive', 0.6239), ('neutral', 0.0))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"Today is finally my day off! The weather is amazing and I'm going to the beach\"\n",
    "s2 = \"Today is finally my day off! The weather is [] and I'm going to the beach\"\n",
    "valency(s1), valency(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "As can be seen in the example above accuracy is not great so a diiferent classisifier is needed, possibly trained on NLTK moview reviews corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train classifier to assing one of the Brown corpus categories to an arbitrary text (test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "categories = ['adventure', 'hobbies', 'humor', 'mystery', 'romance']\n",
    "#cfd = nltk.ConditionalFreqDist((genre, word) for genre in brown.categories() for word in brown.words(categories=genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = nltk.FreqDist([lemmatizer.lemmatize(word) for word in brown.words(categories='humor')\n",
    "                        if word.isalnum() and word.lower() not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 87),\n",
       " ('one', 65),\n",
       " ('would', 56),\n",
       " ('time', 50),\n",
       " ('thing', 40),\n",
       " ('even', 38),\n",
       " ('like', 34),\n",
       " ('could', 30),\n",
       " ('way', 29),\n",
       " ('year', 29)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist2 = nltk.FreqDist([lemmatizer.lemmatize(word) for word in brown.words(categories='mystery')\n",
    "                    if word.isalnum() and word.lower() not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 202),\n",
       " ('would', 186),\n",
       " ('one', 175),\n",
       " ('back', 157),\n",
       " ('could', 141),\n",
       " ('like', 136),\n",
       " ('man', 106),\n",
       " ('get', 101),\n",
       " ('know', 93),\n",
       " ('time', 87)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist2.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnsplit",
   "language": "python",
   "name": "nnsplit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
